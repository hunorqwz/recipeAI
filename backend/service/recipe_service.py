from config import OLLAMA_MODEL, OLLAMA_SYSTEM_PROMPT
from model.recipe_request import RecipeRequest
from langchain_ollama import ChatOllama
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage


class RecipeService:

    LLM = ChatOllama(model=OLLAMA_MODEL, temperature=0.6)

    def get_recipe(self, recipe_request: RecipeRequest) -> BaseMessage:
        """
        Generates a prompt for the receipt and sends it to the language model to generate a response.

        Args:
            recipe_request (RecipeRequest): The receipt request containing the ingredients and type.

        Returns:
            str: The response generated by the language model.
        """
        messages = [
            SystemMessage(content=OLLAMA_SYSTEM_PROMPT),
            HumanMessage(content=self.create_prompt_for_receipt(recipe_request))
        ]
        return self.LLM.invoke(messages)

    @staticmethod
    def create_prompt_for_receipt(receipt_request: RecipeRequest) -> str:
        """
        Generates a prompt for the receipt based on the ingredients and type provided in the receipt request.

        Args:
            receipt_request (RecipeRequest): The receipt request containing the ingredients and type.

        Returns:
            str: A formatted string representing the prompt.
        """
        ingredients_string: str = ", ".join(receipt_request.ingridients)
        return f"Ingredients: {ingredients_string}\nType: {receipt_request.type.value}"